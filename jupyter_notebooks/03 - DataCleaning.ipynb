{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Evaluate missing data\n",
        "* Clean the data\n",
        "* Split data into train and test sets\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/house_prices_records.csv\n",
        "* outputs/datasets/collection/inherited_houses.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Collected Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the collected house price record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"outputs/datasets/collection/house_prices_records.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Review missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no variables with missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the correlation heatmaps and predictive power scores between all the variables\n",
        "\n",
        "Custom function from Code Institute Walkthrough Project 02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    \"\"\" To display correlation heatmaps \"\"\"\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='plasma',\n",
        "                    annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    \"\"\" To display PPS heatmap \"\"\"\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r',\n",
        "                         annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    \"\"\" To calculate Spearman and Pearson correlations and PPS score \"\"\"\n",
        "    df_corr_spearman = df.corr(method=\"spearman\")\n",
        "    df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(\n",
        "        columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\n",
        "        \"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\n",
        "        \"PPS threshold - check PPS score IQR to decide threshold for heatmap\\n\"\n",
        "         )\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman,\n",
        "                      pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "    \"\"\" To display correlation and PPS heatmaps \"\"\"\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable for your ML models are \"\n",
        "          \"correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-colinearity, that is, how the features are \"\n",
        "          \"correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold,\n",
        "                 figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous \"\n",
        "          \"variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold,\n",
        "                 figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two \"\n",
        "          f\"columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect \"\n",
        "          f\"predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize,\n",
        "                font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
        "                  df_corr_spearman=df_corr_spearman,\n",
        "                  pps_matrix=pps_matrix,\n",
        "                  CorrThreshold=0.4, PPS_Threshold=0.2,\n",
        "                  figsize=(12, 10), font_annot=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handling Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load data into a new variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CSV data into a new variable to avoid accidental overwriting\n",
        "df_missing = pd.read_csv(df_raw_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2ndFlrSF\n",
        "\n",
        "- There are 86 missing data entries, equating to 5.9%\n",
        "- Reasons for missing data:\n",
        "    - There is a possibility that some houses do not have a second floor - 53.5% of data is a zero\n",
        "    - Data may have not been inputed, due to user error\n",
        "\n",
        "There are two other variables:\n",
        "1. GrLivArea - which is the total above ground living area square feet\n",
        "2. 1stFlrSF - which is the total first floor square feet\n",
        "\n",
        "All three variables are in square feet. Therefore for any missing data, where the data was not entered due to error. We input the difference between the two variables GrLivArea and 1stFlrSF. Any negative values will be replaced with a zero. There were not missing data after this step, however if there were than the value of zero should be inputed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Display number of missing values in '2ndFlrSF\n",
        "print(\"Initial missing data in '2ndFlrSF':\\n\", df_missing['2ndFlrSF'].isna().sum())\n",
        "\n",
        "# Replace missing values in '2ndFlrSF'\n",
        "df_missing['2ndFlrSF'] = df.apply(\n",
        "    lambda row: row['GrLivArea'] - row['1stFlrSF'] if pd.isna(row['2ndFlrSF']) else row['2ndFlrSF'], axis=1\n",
        ")\n",
        "\n",
        "# Display number of missing values in '2ndFlrSF\n",
        "print(\"Updated missing data in '2ndFlrSF':\\n\", df_missing['2ndFlrSF'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BedroomAbvGr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- There are 99 missing data entries, equating to 6.8%\n",
        "- Reasons for missing data:\n",
        "    - There is a possibility that some houses do not have a bedrooms above ground\n",
        "    - Data may have not been inputed, due to user error\n",
        "\n",
        "Given there is a low number of zeros, we believe that it is unlikely that there are no bedrooms above ground. Therfore we have replaced all missing values with the median number of 3 bedrooms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'BedroomAbvGr'\n",
        "print(\"Initial missing data in 'BedroomAbvGr':\\n\", df_missing['BedroomAbvGr'].isna().sum())\n",
        "\n",
        "# Replace missing values with '3'\n",
        "df_missing['BedroomAbvGr'] = df['BedroomAbvGr'].fillna(3)\n",
        "\n",
        "# Display number of missing values in 'BedroomAbvGr'\n",
        "print(\"Updated missing data in 'BedroomAbvGr':\\n\", df_missing['BedroomAbvGr'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BsmtFinType1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'' = IF TotalBsmtSF > 0, then Unf, ELSE None\n",
        "\n",
        "- There are 114 missing data entries, equating to 7.8%\n",
        "- Reasons for missing data:\n",
        "    - Some houses do not have basements and there was an error in inputting the correct value or decision to purposefully leave as blank.\n",
        "    - There is an error in putting in the value despite there being a basement\n",
        "\n",
        "If there is a basement identified by TotalBsmtSF > 0 (i.e. there is document squrae foot of basement), we will replace the missing data with 1 ('Unfinished'). If there is no document squae foot of basement then we will replace missing data with 0 ('No basement')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'BsmtFinType1'\n",
        "print(\"Initial missing data in 'BsmtFinType1':\\n\", df_missing['BsmtFinType1'].isna().sum())\n",
        "\n",
        "# Replace missing values in 'BsmtFinType1'\n",
        "df_missing['BsmtFinType1'] = df.apply(\n",
        "    lambda row: 1 if pd.isna(row['BsmtFinType1']) and row['TotalBsmtSF'] > 0 else (0 if pd.isna(row['BsmtFinType1']) else row['BsmtFinType1']), axis=1\n",
        ")\n",
        "\n",
        "# Display number of missing values in 'BsmtFinType1'\n",
        "print(\"Updated missing data in 'BsmtFinType1':\\n\", df_missing['BsmtFinType1'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EnclosedPorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- There are 1324 missing data entries, equating to 90.7%\n",
        "- Reasons for missing data:\n",
        "    - There is a possibility that the majority of houses do not have an enclosed porch\n",
        "    - Data may have not been inputed, due to user error\n",
        "\n",
        "There is another variable OpenPorchSF, which also has a higher number of zero but no missing data.\n",
        "There are no observered correlations between this variable and others as well as no stong correlation to SalePrice.\n",
        "\n",
        "As a result due to the large volume of missing data, this variable has been dropped from the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'EnclosedPorch'\n",
        "print(\"Initial missing data in 'EnclosedPorch':\\n\", df_missing['EnclosedPorch'].isna().sum())\n",
        "\n",
        "# Drop column from dataset\n",
        "df_missing.drop(columns=['EnclosedPorch'], inplace=True)\n",
        "\n",
        "# Check column no longer exists\n",
        "df_missing.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GarageFinish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'GarageFinish' = IF GarageArea > 0, then Unf, Else None\n",
        "- There are 162 missing data entries, equating to 11.1%\n",
        "- Reasons for missing data:\n",
        "    - There is a possibility that some houses do not have a garage, therefore this data was not entered\n",
        "    - Data may have been missed despite a garage being present\n",
        "\n",
        "There is another varibale that provides GarageArea which has no missing data. We will apply similar logic to previous varibles where if the GarageArea > 0 then we will replace missing data to '1' (Unfinished). If there is no garage (i.e. GarageArea <0), we will replace missing data with \"0\" (no garage)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'GarageFinish\n",
        "print(\"Initial missing data in 'GarageFinish':\\n\", df_missing['GarageFinish'].isna().sum())\n",
        "\n",
        "# Replace missing values in 'GarageFinish'\n",
        "df_missing['GarageFinish'] = df.apply(\n",
        "    lambda row: 1 if pd.isna(row['GarageFinish']) and row['GarageArea'] > 0 else (0 if pd.isna(row['GarageFinish']) else row['GarageFinish']), axis=1\n",
        ")\n",
        "\n",
        "# Display number of missing values in 'GarageFinish\n",
        "print(\"Updated missing data in 'GarageFinish':\\n\", df_missing['GarageFinish'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GarageYrBlt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'GarageYrBlt' = IF GarageFinish == None (i.e. no garage), then .... , ELSE YearBuilt\n",
        "- There are 81 missing data entries, equating to 5.5%\n",
        "- Reasons for missing data:\n",
        "    - We know some houses do not have garages, therefore this data may have intentionally been missed\n",
        "    - Data may have not been inputed, due to user error\n",
        "\n",
        "There is a strong correlation between the house year build (YearBuilt) and the garage year build (GarageYrBlt). Spearman and Pearson analysis both demonstrate strong correlations with figures of 0.89 and 0.83. The YearBuilt variable has a high PPS score of 0.7 for GarageYrBlt.\n",
        "\n",
        "Given the above, using similar logic to above, if there is a garage present then we will replace any missing values with the YearBuilt. If no garage is present then we will replace with value of '0' This will need to be taken into consideration in future steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'GarageYrBlt'\n",
        "print(\"Initial missing data in 'GarageYrBlt':\\n\", df_missing['GarageYrBlt'].isna().sum())\n",
        "\n",
        "# Replace missing values in 'GarageYrBlt'\n",
        "df_missing['GarageYrBlt'] = df.apply(\n",
        "    lambda row: '0' if pd.isna(row['GarageYrBlt']) and row['GarageArea'] < 0 else (row['YearBuilt'] if pd.isna(row['GarageYrBlt']) else row['GarageYrBlt']), axis=1\n",
        ")\n",
        "\n",
        "# Display number of missing values in 'GarageFinish\n",
        "print(\"Updated missing data in 'GarageYrBlt':\\n\", df_missing['GarageYrBlt'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LotFrontage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'LotFrontage' = Assume none is missing, therefore 0\n",
        "- There are 259 missing data entries, equating to 17.7%\n",
        "- Reasons for missing data:\n",
        "    - It is likely that this data was missed in error\n",
        "\n",
        "Lot frontage is the length of the lot that is facing a street. Assuming that a pot is equal in length and width, then the lot frontage will be the square root of the lot area. I.e. if a lot is 100ft x 100ft then the LotFrontage is 100ft and the LotArea is 10000 squared foot.\n",
        "\n",
        "We know there is no missing data from the LotArea variable, however we cannot make the assumption that all plots of land are square. The data for the LotFrontage appears normally distributed (ignoring the missing data points).\n",
        "\n",
        "Therefore, we will replace all missing data with the mean LotFrontage value of 70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'LotFrontage'\n",
        "print(\"Initial missing data in 'LotFrontage':\\n\", df_missing['LotFrontage'].isna().sum())\n",
        "\n",
        "# Replace missing values with '70'\n",
        "df_missing['LotFrontage'] = df['LotFrontage'].fillna(70)\n",
        "\n",
        "# Display number of missing values in 'LotFrontage'\n",
        "print(\"Updated missing data in 'LotFrontage':\\n\", df_missing['LotFrontage'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MasVnrArea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- There are 8 missing data entries, equating to 0.5%\n",
        "- Reasons for missing data:\n",
        "    - Likely missed due to error\n",
        "\n",
        "Given there is a large number of zeros, suggesting that many of the houses do not have this features. We will assume the missinf data has been missed in error. We will replace all missing values with '0'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'MasVnrArea'\n",
        "print(\"Initial missing data in 'MasVnrArea':\\n\", df_missing['MasVnrArea'].isna().sum())\n",
        "\n",
        "# Replace missing values with '0'\n",
        "df_missing['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n",
        "\n",
        "# Display number of missing values in 'MasVnrArea'\n",
        "print(\"Updated missing data in 'MasVnrArea':\\n\", df_missing['MasVnrArea'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## WoodDeckSF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- There are 1305 missing data entries, equating to 89.4%\n",
        "- Reasons for missing data:\n",
        "    - There is a possibility that some houses do not have any wood decking\n",
        "    - Data may have not been inputed, due to user error\n",
        "\n",
        "There is a large number of missing data. There are no clear correlations between WooDeckSF and SalePrice. There are not clear correlations between WooDeckSf and other variables. As a result we will drop this variable from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display number of missing values in 'WoodDeckSF'\n",
        "print(\"Initial missing data in 'WoodDeckSF':\\n\", df_missing['WoodDeckSF'].isna().sum())\n",
        "\n",
        "# Drop column from dataset\n",
        "df_missing.drop(columns=['WoodDeckSF'], inplace=True)\n",
        "\n",
        "# Check column no longer exists\n",
        "df_missing.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Review Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following the above data review, we have removed 2 variables and their columns of data. We have also replaced all missing data.\n",
        "\n",
        "We can see that we have float, integer and object types.\n",
        "On review:\n",
        "- There does not seem to be any float type entries (i.e. no decimal points).\n",
        "- Object types, after replacing missing, are appear to be integers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the following code to confirm that all float types are in fact integers and convert these to integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the .is_integer function we check each float type\n",
        "def check_integer(df):\n",
        "    cols_to_check = ['2ndFlrSF','BedroomAbvGr','BsmtFinType1','GarageFinish','GarageYrBlt','LotFrontage','MasVnrArea',]\n",
        "\n",
        "    for col in cols_to_check:\n",
        "        if df[col].apply(float.is_integer).all():\n",
        "            return f\"All floats are listed as integers\"\n",
        "        else:\n",
        "            return \"There are some floats listed as floats\"\n",
        "\n",
        "check_integer(df_missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_as_float_type = ['2ndFlrSF','BedroomAbvGr','BsmtFinType1','GarageFinish','GarageYrBlt','LotFrontage','MasVnrArea',]\n",
        "\n",
        "df_missing[variables_as_float_type] = df_missing[variables_as_float_type].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Missing Data Recheck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "check_missing_data = df_missing.columns[df_missing.isna().sum() > 0].to_list()\n",
        "\n",
        "from ydata_profiling import ProfileReport\n",
        "if check_missing_data:\n",
        "    profile = ProfileReport(df=df_missing, minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no variables with missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Split Test and Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = df_missing\n",
        "\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['SalePrice'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate missing data function taken from Code Institute walkthrough project 2 to confirm there are no missing data in the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
        "    df_missing_data = (pd.DataFrame(\n",
        "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                   \"DataType\": df.dtypes}\n",
        "                                    )\n",
        "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
        "                          .query(\"PercentageOfDataset > 0\")\n",
        "                          )\n",
        "\n",
        "    return df_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate missing data in inherited houses dataset after loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw_path_inherited = \"outputs/datasets/collection/inherited_houses.csv\"\n",
        "df_inherited = pd.read_csv(df_raw_path_inherited)\n",
        "df_inherited.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_data_inherited = EvaluateMissingData(df_inherited)\n",
        "print(f\"* There are {df_missing_data_inherited.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data_inherited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Push cleaned data to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/cleaned') # create outputs/datasets/collection folder\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
